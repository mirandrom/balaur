{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/network/scratch/m/mirceara/.cache/huggingface/transformers\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/network/scratch/m/mirceara/.cache/huggingface/datasets\"\n",
    "os.environ[\"BALAUR_CACHE\"] = \"/network/scratch/m/mirceara/.cache/balaur\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/mila/m/mirceara/balaur/experiments/pretrain/\")\n",
    "from run_bort import MlmModel, MlmModelConfig, MlmWnreDataModule, MlmWnreDataModuleConfig, WNRE\n",
    "from balaur.modeling.backbone.bort import BortForSequenceClassification\n",
    "from balaur import ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "import math\n",
    "from pathlib import Path\n",
    "import random\n",
    "import json\n",
    "\n",
    "from itertools import chain\n",
    "import copy\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import datasets as ds\n",
    "import transformers as tr\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bort_model(ckpt_dir: Path, step: int, num_labels: int = 2, **config_kwargs):\n",
    "    \n",
    "    p = list(ckpt_dir.glob(f\"*step={step}.ckpt\"))\n",
    "    assert len(p) == 1, f\"Unresolvable paths: {p}\"\n",
    "    p = p[0]\n",
    "    \n",
    "    ckpt = torch.load(p)\n",
    "    config = MlmModelConfig()\n",
    "    config.__dict__.update(ckpt['hyper_parameters'])\n",
    "    config.__dict__.update(config_kwargs)\n",
    "    pl_model = MlmModel(config=config)\n",
    "    pl_model.load_state_dict(ckpt['state_dict'])\n",
    "    \n",
    "    model = BortForSequenceClassification(pl_model.bort_config, num_labels=num_labels)\n",
    "    model.bort = pl_model.model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = tr.AutoTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "snli = ds.load_dataset('snli', split='train').filter(lambda x: x['label'] != -1)\n",
    "snli = snli.rename_column('premise', 'sentence1').rename_column('hypothesis', 'sentence2')\n",
    "snli_class_label = snli.features['label']\n",
    "\n",
    "\n",
    "d = ds.DatasetDict(dict(\n",
    "            nmonli = ds.load_dataset(\"json\", data_files=\"monli/nmonli_train.jsonl\", split='train'),\n",
    "            pmonli = ds.load_dataset(\"json\", data_files=\"monli/pmonli.jsonl\", split='train'),\n",
    "        ))\n",
    "d = d.map(lambda x: {'label': [snli_class_label.str2int(l) for l in x['gold_label']]}, batched=True)\n",
    "d = d.cast_column('label', snli_class_label)\n",
    "# d['snli'] = snli\n",
    "d = d.rename_column('label', 'labels')\n",
    "\n",
    "d = d.map(lambda x: tknzr(x['sentence1'], x['sentence2'], return_token_type_ids=False), batched=True)\n",
    "\n",
    "d['nmonli'] = d['nmonli'].add_column('source', ['nmonli' for _ in range(len(d['nmonli']))])\n",
    "d['pmonli'] = d['pmonli'].add_column('source', ['pmonli' for _ in range(len(d['pmonli']))])\n",
    "# d['snli'] = d['snli'].add_column('source', ['snli' for _ in range(len(d['snli']))])\n",
    "# d['snli'] = d['snli'].add_column('depth', [None for _ in range(len(d['snli']))])\n",
    "\n",
    "\n",
    "max_lengths = [max([len(x) for x in _d['input_ids']]) for _d in d.values()]\n",
    "max_length = max(max_lengths)\n",
    "d = d.map(lambda e: tknzr.pad(e, padding='max_length', max_length=max_length), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnre = WNRE('roberta-base', rel_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_wordnet(tok: str):\n",
    "    tok = tknzr.vocab.get(\"Ä \" +tok, None)\n",
    "    if tok:\n",
    "        return bool(wnre.tok2rel2syn[tok])\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def annotate_single_token(example):\n",
    "        if example['source'] == 'snli':\n",
    "            return dict(\n",
    "                hypernym_in_lexrel=None,\n",
    "                hyponym_in_lexrel=None,\n",
    "            )\n",
    "        if example['depth'] > 0:\n",
    "            hypernym =  example['sentence2_lex']\n",
    "            hyponym =  example['sentence1_lex']\n",
    "        else:\n",
    "            hypernym =  example['sentence1_lex']  \n",
    "            hyponym =  example['sentence2_lex']\n",
    "            \n",
    "        return dict(\n",
    "            hypernym_in_lexrel=is_in_wordnet(hypernym),\n",
    "            hyponym_in_lexrel=is_in_wordnet(hyponym),\n",
    "        )\n",
    "\n",
    "d = d.map(annotate_single_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-eval split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def systematic_train_eval_split(monli_ds: ds.Dataset, seed: int, frac=0.8):\n",
    "    monli_dict = monli_ds.to_dict()\n",
    "    hypernym_dict = defaultdict(int)\n",
    "    for i in range(len(monli_ds)):\n",
    "        if monli_dict['depth'][i] > 0:\n",
    "            hypernym =  monli_dict['sentence2_lex'][i]\n",
    "        else:\n",
    "            hypernym =  monli_dict['sentence1_lex'][i]             \n",
    "        hypernym_dict[hypernym] += 1\n",
    "    \n",
    "    hypernyms = list(hypernym_dict.keys())\n",
    "    random.Random(seed).shuffle(hypernyms)\n",
    "    num_train = int(len(hypernyms) * frac)\n",
    "    train_hypernyms = set(hypernyms[:num_train])\n",
    "    eval_hypernyms = set(hypernyms[num_train:])\n",
    "    \n",
    "    train_dict = defaultdict(list)\n",
    "    eval_dict = defaultdict(list)\n",
    "    for i in range(len(monli_ds)):\n",
    "        if monli_dict['depth'][i] > 0:\n",
    "            hypernym =  monli_dict['sentence2_lex'][i]\n",
    "        else:\n",
    "            hypernym =  monli_dict['sentence1_lex'][i]   \n",
    "            \n",
    "        if hypernym in train_hypernyms:\n",
    "            for k in monli_dict.keys():\n",
    "                train_dict[k].append(monli_dict[k][i])\n",
    "        else:\n",
    "            for k in monli_dict.keys():\n",
    "                eval_dict[k].append(monli_dict[k][i])\n",
    "    \n",
    "    train_ds = ds.Dataset.from_dict(train_dict)\n",
    "    eval_ds = ds.Dataset.from_dict(eval_dict)\n",
    "    return train_ds, eval_ds\n",
    "\n",
    "@lru_cache\n",
    "def get_train_eval_sets(seed, frac=0.8):\n",
    "    keep_cols = ['input_ids', 'labels','attention_mask',\n",
    "                'source', 'hypernym_in_lexrel', 'hyponym_in_lexrel',\n",
    "                'depth']\n",
    "    train_nmonli, eval_nmonli = systematic_train_eval_split(d['nmonli'], seed=seed, frac=frac)\n",
    "    train_pmonli, eval_pmonli = systematic_train_eval_split(d['pmonli'], seed=seed, frac=frac)\n",
    "\n",
    "    train_nmonli = train_nmonli.remove_columns([c for c in train_nmonli.column_names if c not in keep_cols])\n",
    "    train_pmonli = train_pmonli.remove_columns([c for c in train_pmonli.column_names if c not in keep_cols])\n",
    "\n",
    "    train_dataset = ds.concatenate_datasets([train_nmonli, train_pmonli])\n",
    "    train_dataset = train_dataset.shuffle(seed=seed)\n",
    "    eval_dataset = ds.concatenate_datasets([eval_nmonli, eval_pmonli])\n",
    "    \n",
    "    train_dataset.set_format(type='torch', columns=['input_ids', 'labels', 'attention_mask'], output_all_columns=True)\n",
    "    eval_dataset.set_format(type='torch', columns=['input_ids', 'labels', 'attention_mask'], output_all_columns=True)\n",
    "    \n",
    "    return train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset = get_train_eval_sets(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoNLIMetrics:\n",
    "    def __init__(self):\n",
    "        self.acc = defaultdict(list)\n",
    "        self.nll = defaultdict(list)\n",
    "        self.labels = defaultdict(list)\n",
    "        self.losses = []\n",
    "\n",
    "    def update(self,\n",
    "               logits: torch.Tensor,    # (bsz,vsz)\n",
    "               labels: torch.Tensor, # (bsz, nlabels), nlabels varies within a batch\n",
    "               loss: float,\n",
    "               sources: List[str],\n",
    "               depths: List[int],\n",
    "               hypernym_in_lexrel: List[bool],\n",
    "               hyponym_in_lexrel: List[bool],\n",
    "               ):\n",
    "        \n",
    "        nll_tensor = -F.log_softmax(logits, dim=-1)\n",
    "        for bidx, vidx in enumerate(labels):\n",
    "            nlls = nll_tensor[bidx][vidx].item()\n",
    "            accs = (nlls <= nll_tensor[bidx]).all()\n",
    "\n",
    "            source = sources[bidx]\n",
    "            depth = 0 if depths[bidx] is None else abs(depths[bidx]) \n",
    "            if source == 'snli':\n",
    "                k = source\n",
    "            elif hypernym_in_lexrel[bidx] and hyponym_in_lexrel[bidx]:\n",
    "                k = f\"{source}_both_{depth}\"\n",
    "            elif hypernym_in_lexrel[bidx]:\n",
    "                k = f\"{source}_hyper_{depth}\"\n",
    "            elif hyponym_in_lexrel[bidx]:\n",
    "                k = f\"{source}_hypo_{depth}\"\n",
    "            else:\n",
    "                k = f\"{source}_none_{depth}\"\n",
    "                \n",
    "            self.nll[k].append(nlls)\n",
    "            self.nll[\"all\"].append(nlls)\n",
    "            self.acc[k].append(int(accs))\n",
    "            self.acc[\"all\"].append(int(accs))\n",
    "            self.labels[k].append(labels[bidx])\n",
    "            self.labels[\"all\"].append(labels[bidx])\n",
    "                \n",
    "        self.losses += [loss]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_labels_collate(batch):\n",
    "    tensor_batch = [{k: x[k] for k in ['input_ids', 'attention_mask', 'labels']} for x in batch]\n",
    "    out = torch.utils.data.dataloader.default_collate(tensor_batch)\n",
    "    out['source'] = [x['source'] for x in batch]\n",
    "    out['hypernym_in_lexrel'] = [x['hypernym_in_lexrel'] for x in batch]\n",
    "    out['hyponym_in_lexrel'] = [x['hyponym_in_lexrel'] for x in batch]\n",
    "    out['depth'] = [x['depth'] for x in batch]\n",
    "    return out\n",
    "\n",
    "\n",
    "def eval_loop(*, model: BortForSequenceClassification, \n",
    "              tokenizer: tr.AutoTokenizer, \n",
    "              dataset: ds.Dataset, \n",
    "              bsz: int,\n",
    "              device: torch.device = None,\n",
    "             ):\n",
    "    # setup\n",
    "    device = device or torch.device('cpu')\n",
    "    model.to(device)\n",
    "    dl = torch.utils.data.DataLoader(dataset, batch_size=bsz, shuffle=False, drop_last=False, \n",
    "                                     collate_fn=eval_labels_collate)\n",
    "    metrics = MoNLIMetrics()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch in dl:\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                out = model(\n",
    "                    batch['input_ids'].to(device),\n",
    "                    labels=batch['labels'].to(device),\n",
    "                    attention_mask=batch['attention_mask'].to(device)\n",
    "                )\n",
    "            metrics.update(\n",
    "                out['logits'],\n",
    "                batch['labels'],\n",
    "                out['loss'].item(),\n",
    "                sources=batch['source'],\n",
    "                depths=batch['depth'],\n",
    "                hypernym_in_lexrel=batch['hypernym_in_lexrel'],\n",
    "                hyponym_in_lexrel=batch['hyponym_in_lexrel'],\n",
    "            )\n",
    "    return metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ft_labels_collate(batch):\n",
    "    tensor_batch = [{k: x[k] for k in ['input_ids', 'attention_mask', 'labels']} for x in batch]\n",
    "    out = torch.utils.data.dataloader.default_collate(tensor_batch)\n",
    "    out['source'] = [x['source'] for x in batch]\n",
    "    out['hypernym_in_lexrel'] = [x['hypernym_in_lexrel'] for x in batch]\n",
    "    out['hyponym_in_lexrel'] = [x['hyponym_in_lexrel'] for x in batch]\n",
    "    out['depth'] = [x['depth'] for x in batch]\n",
    "    return out\n",
    "\n",
    "\n",
    "def finetune_loop(*, model: BortForSequenceClassification, tokenizer: tr.AutoTokenizer,\n",
    "                  bsz: int = 16, lr: float = 1e-5, wd: float = 0.1, max_grad_norm=1.0,\n",
    "                  warmup: float = 0.06,\n",
    "                  device: torch.device = None,\n",
    "                  num_seeds: int = 5, start_seed: int = 42,\n",
    "                  max_train_examples: int = 4096,\n",
    "                  eval_every_n_examples: int = 512,\n",
    "                  betas: list[float] = [0.9, 0.98],\n",
    "                 ):\n",
    "    \n",
    "    device = device or torch.device('cpu')\n",
    "    train_metrics = {}\n",
    "    eval_metrics = {}\n",
    "    losses = {}\n",
    "    seed = start_seed\n",
    "    \n",
    "    training_steps = math.ceil(max_train_examples / bsz)\n",
    "    warmup_steps = int(training_steps * warmup)\n",
    "    for seed_incr in range(num_seeds):\n",
    "        seed_model = copy.deepcopy(model)\n",
    "        seed_model.train()\n",
    "        seed_model = seed_model.to(device)\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in seed_model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": wd,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in seed_model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=lr, weight_decay=wd, betas=betas)\n",
    "        lr_scheduler = tr.get_scheduler('linear', optimizer, warmup_steps, training_steps)\n",
    "        \n",
    "        seed = seed + seed_incr\n",
    "        train_dataset, eval_dataset = get_train_eval_sets(seed)\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed)\n",
    "        dl = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=bsz, shuffle=True, drop_last=False, \n",
    "            generator=g, collate_fn=ft_labels_collate,\n",
    "        )\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        train_metrics[seed] = {}\n",
    "        eval_metrics[seed] = {}\n",
    "        seen = 0\n",
    "        next_eval_seen = 0\n",
    "        while seen < max_train_examples:\n",
    "            pbar = tqdm(dl, total=len(dl), desc=f\"Training (loss=0)\")\n",
    "            for batch in pbar:\n",
    "                with torch.autocast(device_type='cuda', dtype=torch.float32):\n",
    "                    optimizer.zero_grad()\n",
    "                    out = seed_model(\n",
    "                        batch['input_ids'].to(device),\n",
    "                        labels=batch['labels'].to(device),\n",
    "                        attention_mask=batch['attention_mask'].to(device)\n",
    "                    )\n",
    "                if seen >= next_eval_seen:\n",
    "                    next_eval_seen += eval_every_n_examples\n",
    "                    train_metric = MoNLIMetrics()\n",
    "                    train_metric.update(\n",
    "                        out['logits'],\n",
    "                        batch['labels'],\n",
    "                        out['loss'].item(),\n",
    "                        sources=batch['source'],\n",
    "                        depths=batch['depth'],\n",
    "                        hypernym_in_lexrel=batch['hypernym_in_lexrel'],\n",
    "                        hyponym_in_lexrel=batch['hyponym_in_lexrel'],\n",
    "                    )\n",
    "                    train_metrics[seed][seen] = train_metric\n",
    "                    eval_metrics[seed][seen] = eval_loop(\n",
    "                        model=seed_model,\n",
    "                        tokenizer=tokenizer, \n",
    "                        dataset=eval_dataset,\n",
    "                        bsz=32, \n",
    "                        device=device,\n",
    "                    )\n",
    "                loss = out['loss']\n",
    "                pbar.set_description(f\"Training (loss={loss})\")\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(seed_model.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                seen += bsz\n",
    "                if seen >= max_train_examples:\n",
    "                    break \n",
    "    return train_metrics, eval_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAMES = [\"mlm_only\", \"mlm_wnre\"]\n",
    "STEP = 25_000\n",
    "exp_dir = Path(\"/network/scratch/m/mirceara/.cache/balaur/runs\")\n",
    "\n",
    "models = {\n",
    "    m: load_bort_model(Path(f\"{exp_dir}/{m}/balaur/{m}/checkpoints/\"), STEP, num_labels=2)\n",
    "    for m in MODEL_NAMES\n",
    "}\n",
    "tknzr = tr.AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = {}\n",
    "eval_metrics = {}\n",
    "num_epochs = 10\n",
    "for m in  [\"mlm_wnre\", \"mlm_only\"]:\n",
    "    model = models[m]\n",
    "    train_metrics[m], eval_metrics[m] = finetune_loop(\n",
    "        model=model, \n",
    "        tokenizer=tknzr, \n",
    "        bsz=32,\n",
    "        lr=8e-5,\n",
    "        device=torch.device('cuda'),\n",
    "        num_seeds=5,\n",
    "        max_train_examples=len(train_dataset)*num_epochs,\n",
    "        eval_every_n_examples=int(len(train_dataset)/2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"train_metrics.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_metrics, f)\n",
    "    \n",
    "with open(\"eval_metrics.pkl\", \"wb\") as f:\n",
    "    pickle.dump(eval_metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from IPython.core.display import display, HTML\n",
    "from statistics import quantiles\n",
    "import plotly.io as pio\n",
    "\n",
    "\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorband_trace(x, y, ylo, yhi, legend: str, rgb='0,100,80', showlegend: bool = True):\n",
    "    trace = [\n",
    "        go.Scatter(\n",
    "            name=legend,\n",
    "            x=x,\n",
    "            y=y,\n",
    "            line=dict(color=f'rgb({rgb})'),\n",
    "            mode='lines',\n",
    "            showlegend=showlegend,\n",
    "        ),\n",
    "        go.Scatter(\n",
    "            x=x+x[::-1], # x, then x reversed\n",
    "            y=yhi+ylo[::-1], # upper, then lower reversed\n",
    "            fill='toself',\n",
    "            fillcolor=f'rgba({rgb},0.2)',\n",
    "            line=dict(color='rgba(255,255,255,0)'),\n",
    "            hoverinfo=\"skip\",\n",
    "            showlegend=False\n",
    "        )\n",
    "    ]\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nll_traces(metrics: MoNLIMetrics, model1: str, model2: str, stratification_key: str, rgb1: str='45,114,178', rgb2: str='194,24,7', showlegend: bool = True):\n",
    "    traces = []\n",
    "    for obj, rgb in zip([model1, model2], [rgb1,rgb2]):\n",
    "        x = []\n",
    "        y = []\n",
    "        ylo = []\n",
    "        yhi = []\n",
    "        y_mean = []\n",
    "        x_mean = []\n",
    "        nlls = defaultdict(list)\n",
    "        for seed in metrics[obj]:\n",
    "            for step, metric in metrics[obj][seed].items():\n",
    "                nlls[step].extend(metric.nll[stratification_key])\n",
    "        for step in sorted(metrics[obj][seed]):\n",
    "            model = f\"{obj}_{step}\"\n",
    "            nll = [np.mean(xx) for xx in nlls[step]]\n",
    "            if len(nll) >= 2:\n",
    "                quartiles = quantiles(nll, n=4)\n",
    "                x.append(step)\n",
    "                y.append(quartiles[1])\n",
    "                ylo.append(quartiles[0])\n",
    "                yhi.append(quartiles[2])\n",
    "            if len(nll):\n",
    "                y_mean.append(np.mean(nll))\n",
    "                x_mean.append(step)\n",
    "        traces.extend(errorband_trace(x,y,ylo,yhi,obj,rgb, showlegend))\n",
    "        traces.append(\n",
    "            go.Scatter(\n",
    "                x=x,\n",
    "                y=y_mean,\n",
    "                line=dict(color=f'rgb({rgb})', dash='dash'),\n",
    "                name=f\"{obj} (mean)\",\n",
    "                showlegend=showlegend\n",
    "            )\n",
    "        )\n",
    "    return traces\n",
    "\n",
    "\n",
    "def get_fixed_nll_traces(metrics: MoNLIMetrics, model1: str, model2: str, leg1: str, leg2: str, rgb1: str='45,114,178', rgb2: str='194,24,7', stratification_key: str = 'all', showlegend: bool = True):\n",
    "    traces = []\n",
    "    for obj, leg, rgb in zip([model1, model2], [leg1, leg2], [rgb1,rgb2]):\n",
    "        x = []\n",
    "        y = []\n",
    "        ylo = []\n",
    "        yhi = []\n",
    "        y_mean = []\n",
    "        x_mean = []\n",
    "        nlls = defaultdict(list)\n",
    "        for seed in metrics[obj]:\n",
    "            for step, metric in metrics[obj][seed].items():\n",
    "                for key in metric.nll.keys():\n",
    "                    if stratification_key in key:\n",
    "                        nlls[step].extend(metric.nll[key]) \n",
    "        for step in sorted(metrics[obj][seed]):\n",
    "            if len(nlls[step]):\n",
    "                nll = [np.mean(x) for x in nlls[step]]\n",
    "                y_mean_step = np.mean(nll)\n",
    "                y_std_step = np.std(nll)\n",
    "                x.append(step)\n",
    "                y.append(y_mean_step)\n",
    "#                 ylo.append(y_mean_step - y_std_step)\n",
    "#                 yhi.append(y_mean_step + y_std_step)\n",
    "                ylo.append(y_mean_step)\n",
    "                yhi.append(y_mean_step)\n",
    "        traces.extend(errorband_trace(x,y,ylo,yhi,leg,rgb, showlegend))\n",
    "    return traces\n",
    "\n",
    "\n",
    "def get_acc_traces(metrics: MoNLIMetrics, model1: str, model2: str, leg1: str, leg2: str, rgb1: str='45,114,178', rgb2: str='194,24,7', stratification_key: str = 'all', showlegend: bool = True):\n",
    "    traces = []\n",
    "    for obj, leg, rgb in zip([model1, model2], [leg1, leg2], [rgb1,rgb2]):\n",
    "        x = []\n",
    "        y = []\n",
    "        ylo = []\n",
    "        yhi = []\n",
    "        y_mean = []\n",
    "        x_mean = []\n",
    "        accs = defaultdict(list)\n",
    "        for seed in metrics[obj]:\n",
    "            for step, metric in metrics[obj][seed].items():\n",
    "                for key in metric.acc.keys():\n",
    "                    if stratification_key in key:\n",
    "                        accs[step].extend(metric.acc[key]) \n",
    "        for step in sorted(metrics[obj][seed]):\n",
    "            if len(accs[step]):\n",
    "                y_mean_step = np.mean(accs[step])\n",
    "                y_std_step = np.std(accs[step])\n",
    "                x.append(step)\n",
    "                y.append(y_mean_step)\n",
    "#                 ylo.append(y_mean_step - y_std_step)\n",
    "#                 yhi.append(y_mean_step + y_std_step)\n",
    "                ylo.append(y_mean_step)\n",
    "                yhi.append(y_mean_step)\n",
    "        traces.extend(errorband_trace(x,y,ylo,yhi,leg,rgb, showlegend))\n",
    "    return traces\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LEGENDS = {\n",
    "    \"mlm_only\": \"BERT (OURS)\",\n",
    "    \"mlm_wnre\": \"BERT+BALAUR\"\n",
    "}\n",
    "MODEL_RGBS = {\n",
    "    \"mlm_only\": \"255,127,80\",\n",
    "    \"mlm_wnre\": \"65,105,225\"\n",
    "}\n",
    "\n",
    "acc_traces = get_acc_traces(\n",
    "    eval_metrics, \n",
    "    MODEL_NAMES[1], MODEL_NAMES[0],\n",
    "    MODEL_LEGENDS[MODEL_NAMES[1]], MODEL_LEGENDS[MODEL_NAMES[0]],\n",
    "    MODEL_RGBS[MODEL_NAMES[1]], MODEL_RGBS[MODEL_NAMES[0]],\n",
    "    stratification_key='all'\n",
    ")\n",
    "\n",
    "fig = go.Figure(acc_traces)\n",
    "\n",
    "\n",
    "fig.update_yaxes(type=\"linear\")\n",
    "fig.update_xaxes(type=\"linear\")\n",
    "\n",
    "fig.update_layout(showlegend=True, template='simple_white')\n",
    "# fig.update_layout(legend=dict(yanchor='top', xanchor='left', x=0.01,y=0.99))\n",
    "fig.update_layout(legend=dict(yanchor='bottom', xanchor='right', x=0.99,y=0.01))\n",
    "\n",
    "fig.update_layout(width =500, height=250, \n",
    "                  font_family=\"Serif\", \n",
    "                  font_size=12, \n",
    "                  margin_l=5, margin_t=5, margin_b=5, margin_r=5)\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Number of training examples\",\n",
    "    yaxis_title=\"Accuracy (MoNLI)\",\n",
    ")\n",
    "display(HTML(fig.to_html()))\n",
    "pio.write_image(fig, \"ft_monli.pdf\", width=1.5*300, height=0.75*300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_stratified_accs(metrics: MoNLIMetrics, model1: str, model2: str, stratification_key: str):\n",
    "    d = {}\n",
    "    for obj in [model1, model2]:\n",
    "        accs = []\n",
    "        for seed in metrics[obj]:\n",
    "            step = max(metrics[obj][seed].keys())\n",
    "            metric = metrics[obj][seed][step]\n",
    "            accs.extend(metric.acc[stratification_key])\n",
    "            \n",
    "        model = f\"{obj}_{step}\"\n",
    "        acc = 100 * np.mean([np.mean(xx) for xx in accs])\n",
    "        \n",
    "        d[obj] = acc\n",
    "    return d\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "# eval NLL\n",
    "DEPTHS = list(range(1,7)) # technically includes up to depth 10, but plots are very sparse and uninterpretable beyond depth 6\n",
    "SOURCES = ['nmonli', 'pmonli']\n",
    "SINGLE_TOKENS = ['both', 'hyper', 'none']\n",
    "\n",
    "\n",
    "stratified_counts = {}\n",
    "for source in SOURCES:\n",
    "    for depth in DEPTHS:\n",
    "        for single_token in SINGLE_TOKENS:\n",
    "            stratification_key = f\"{source}_{single_token}_{depth}\"\n",
    "            num_examples = 0\n",
    "            for seed in eval_metrics['mlm_wnre']:\n",
    "                num_examples += len(eval_metrics['mlm_wnre'][seed][0].labels[stratification_key])\n",
    "            num_examples /= len(eval_metrics['mlm_wnre'])\n",
    "            num_examples = math.ceil(num_examples)\n",
    "            stratified_counts[stratification_key] = num_examples\n",
    "\n",
    "stratified_accs = {m: {} for m in ['mlm_wnre', 'mlm_only']}\n",
    "for i1, source in enumerate(SOURCES):\n",
    "    for i2, depth in enumerate(DEPTHS):\n",
    "        for i3, single_token in enumerate(SINGLE_TOKENS):\n",
    "            stratification_key = f\"{source}_{single_token}_{depth}\"\n",
    "            accs = get_stratified_accs(eval_metrics, 'mlm_only', 'mlm_wnre', stratification_key)\n",
    "            for m, acc in accs.items():\n",
    "                stratified_accs[m][stratification_key] = acc\n",
    "\n",
    "\n",
    "print(json.dumps(stratified_counts, indent=2))\n",
    "print(json.dumps(stratified_accs, indent=2))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMoNLI performance\n",
    "agg_fns = dict(\n",
    "    nmonli_overall=lambda k: \"nmonli\" in k,\n",
    "    nmonli_both=lambda k: \"nmonli\" in k and \"both\" in k,\n",
    "    nmonli_hyper=lambda k: \"nmonli\" in k and \"hyper\" in k,\n",
    "    nmonli_none=lambda k: \"nmonli\" in k and \"none\" in k,\n",
    "    pmonli_overall=lambda k: \"pmonli\" in k,\n",
    "    pmonli_both=lambda k: \"pmonli\" in k and \"both\" in k,\n",
    "    pmonli_hyper=lambda k: \"pmonli\" in k and \"hyper\" in k,\n",
    "    pmonli_none=lambda k: \"pmonli\" in k and \"none\" in k,\n",
    "\n",
    ")\n",
    "for agg, agg_fn in agg_fns.items():\n",
    "    print()\n",
    "    print(agg)\n",
    "    for m, saccs in stratified_accs.items():\n",
    "        acc_sum = 0\n",
    "        acc_div = 0\n",
    "        for k, sacc in saccs.items():\n",
    "            if agg_fn(k):\n",
    "                c = stratified_counts[k]\n",
    "                if math.isnan(sacc):\n",
    "                    sacc = 0\n",
    "                    assert c == 0\n",
    "\n",
    "                acc_sum += sacc*c\n",
    "                acc_div += c   \n",
    "        acc_avg = acc_sum / acc_div if acc_div else None\n",
    "        print(m, acc_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMoNLI performance\n",
    "agg_fns = dict(\n",
    "    nmonli_1=lambda k: \"nmonli\" in k and \"1\" in k,\n",
    "    nmonli_2=lambda k: \"nmonli\" in k and \"2\" in k,\n",
    "    nmonli_3=lambda k: \"nmonli\" in k and \"3\" in k,\n",
    "    nmonli_4=lambda k: \"nmonli\" in k and \"4\" in k,\n",
    "    nmonli_5=lambda k: \"nmonli\" in k and \"5\" in k,\n",
    "    nmonli_6=lambda k: \"nmonli\" in k and \"6\" in k,\n",
    "    pmonli_1=lambda k: \"pmonli\" in k and \"1\" in k,\n",
    "    pmonli_2=lambda k: \"pmonli\" in k and \"2\" in k,\n",
    "    pmonli_3=lambda k: \"pmonli\" in k and \"3\" in k,\n",
    "    pmonli_4=lambda k: \"pmonli\" in k and \"4\" in k,\n",
    "    pmonli_5=lambda k: \"pmonli\" in k and \"5\" in k,\n",
    "    pmonli_6=lambda k: \"pmonli\" in k and \"6\" in k,\n",
    "\n",
    ")\n",
    "for agg, agg_fn in agg_fns.items():\n",
    "    print()\n",
    "    print(agg)\n",
    "    for m, saccs in stratified_accs.items():\n",
    "        acc_sum = 0\n",
    "        acc_div = 0\n",
    "        for k, sacc in saccs.items():\n",
    "            if agg_fn(k):\n",
    "                c = stratified_counts[k]\n",
    "                if math.isnan(sacc):\n",
    "                    sacc = 0\n",
    "                    assert c == 0\n",
    "\n",
    "                acc_sum += sacc*c\n",
    "                acc_div += c   \n",
    "        acc_avg = acc_sum / acc_div if acc_div else None\n",
    "        print(m, acc_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (emnlp2023)",
   "language": "python",
   "name": "emnlp2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
