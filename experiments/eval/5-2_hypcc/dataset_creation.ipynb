{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/network/scratch/m/mirceara/.cache/huggingface/transformers\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/network/scratch/m/mirceara/.cache/huggingface/datasets\"\n",
    "os.environ[\"BALAUR_CACHE\"] = \"/network/scratch/m/mirceara/.cache/balaur\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/mila/m/mirceara/balaur/experiments/pretrain/\")\n",
    "from run_bort import MlmModel, MlmModelConfig, MlmWnreDataModule, MlmWnreDataModuleConfig, WNRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import datasets as ds\n",
    "import transformers as tr\n",
    "\n",
    "from typing import List, Union\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract hypernymy-related synsets from WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from balaur.wordnet.utils import import_wordnet\n",
    "wn = import_wordnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnre_bert = WNRE('bert-base-uncased', rel_depth=1)\n",
    "wnre_roberta = WNRE('roberta-base', rel_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapping_synsets = set(wnre_bert.synset_to_tokens.keys()).intersection(wnre_roberta.synset_to_tokens.keys())\n",
    "overlapping_tokens = set(wnre_bert.token_to_synsets.keys()).intersection(wnre_roberta.token_to_synsets.keys())\n",
    "\n",
    "synset_to_tokens = {}\n",
    "for s, ts in wnre_bert.synset_to_tokens.items():\n",
    "    if s in overlapping_synsets:\n",
    "        for t in ts:\n",
    "            if t in overlapping_tokens:\n",
    "                synset_to_tokens[s] = synset_to_tokens.get(s, []) + [t]\n",
    "            \n",
    "hypernymy_synsets = {}\n",
    "for s, hs in wnre_bert.related_synsets['hypernymy'].items():\n",
    "    if s in synset_to_tokens:\n",
    "        for h in hs:\n",
    "            if h in synset_to_tokens:\n",
    "                hypernymy_synsets[s] = hypernymy_synsets.get(s, []) + [h]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create examples from extracted hypernym pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "p = inflect.engine()\n",
    "\n",
    "get_num = lambda x: \"plural\" if p.singular_noun(x) else \"singular\"\n",
    "get_article = lambda x: p.a(x).split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example(hypernym: str, hyponym: str, \n",
    "                   mask_token: str = \"[MASK]\", \n",
    "                   **kwargs):\n",
    "    \n",
    "    number_hypernym = get_num(hypernym)\n",
    "    number_hyponym = get_num(hyponym)\n",
    "    hyponym_article = get_article(hyponym)\n",
    "    \n",
    "    out = dict(\n",
    "        hypernym=hypernym,\n",
    "        hyponym=hyponym,\n",
    "        number_hypernym=number_hypernym,\n",
    "        number_hyponym=number_hyponym,\n",
    "    )\n",
    "    for to_mask in ['hypernym', 'hyponym']:\n",
    "        \n",
    "        # replace with maskrel_depth\n",
    "        if to_mask == 'hypernym':\n",
    "            hypernym = mask_token\n",
    "            hyponym = out['hyponym']\n",
    "        elif to_mask == 'hyponym':\n",
    "            hyponym = mask_token\n",
    "            hypernym = out['hypernym']\n",
    "            \n",
    "        # create example from template\n",
    "        if number_hyponym == 'singular' and number_hypernym == 'singular':\n",
    "            text = f\"{hyponym_article} {hyponym} is a type of {hypernym}.\"\n",
    "        elif number_hyponym == 'plural' and number_hypernym == 'singular':\n",
    "            text = f\"{hyponym} are a type of {hypernym}.\"\n",
    "        elif number_hyponym == 'plural' and number_hypernym == 'plural':\n",
    "            text = f\"{hyponym} are types of {hypernym}.\"\n",
    "        elif number_hyponym == 'singular' and number_hypernym == 'plural':\n",
    "            text = f\"types of {hypernym} include {hyponym_article} {hyponym}.\"\n",
    "        out[f\"masked_{to_mask}\"] = text\n",
    "\n",
    "    return out\n",
    "\n",
    "rows = []\n",
    "seen_pairs = set()\n",
    "for hypo_syn, hyper_syns in tqdm(hypernymy_synsets.items()):\n",
    "    hypo_lex = wn.synset(hypo_syn).lexname()\n",
    "    hypo_toks = synset_to_tokens[hypo_syn]\n",
    "    for hyper_syn in hyper_syns:\n",
    "        hyper_lex = wn.synset(hyper_syn).lexname()\n",
    "        hyper_toks = synset_to_tokens[hyper_syn]\n",
    "        for hyper_tok in hyper_toks:\n",
    "            for hypo_tok in hypo_toks:\n",
    "                k = f\"{hyper_tok}, {hypo_tok}\"\n",
    "                if hypo_tok == hyper_tok:\n",
    "                    continue\n",
    "                if k in seen_pairs:\n",
    "                    continue\n",
    "                seen_pairs.add(k)\n",
    "                x = create_example(hyper_tok, hypo_tok)\n",
    "                x.update(dict(\n",
    "                    lex_hypernym=hyper_lex, lex_hyponym=hypo_lex,\n",
    "                    syn_hypernym=hyper_syn, syn_hyponym=hypo_syn\n",
    "                ))\n",
    "                rows.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter noisy examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows)\n",
    "print(f\"Num pairs: {len(df)}\")\n",
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove noun.Tops\n",
    "Tops are top-level entries in the WordNet hierarchy which are often noisy by virtue of their generality (e.g. \"Mortal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.loc[df.lex_hypernym != 'noun.Tops']\n",
    "print(f\"Num pairs: {len(df2)}\")\n",
    "df2.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove disproportionately represented hypernyms\n",
    "Certain hypernyms have a disproportionate number of hyponyms, often due to too-general wordsenses or lemmatization. We manually inspect the most over-represented hypernyms in HypCC to filter noisy hypernym wordforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h,c in df2.hypernym.value_counts()[:100].to_dict().items():\n",
    "    print(h,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we manually look through each of the hypernyms above to identify noisy cases\n",
    "df2.loc[(df2['hypernym'] == 'force')].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually remove examples with hypernyms that are overloaded, unnatural, etc\n",
    "# we limit ourselves to manually inspecting hypernyms that occur in >=50 pairs \n",
    "# Note: wordnet and nltk lemmatization are so atrocious\n",
    "stop_hypers = [\n",
    "    'content',\n",
    "    'contents',\n",
    "    'instrumentation',\n",
    "    'part',\n",
    "    'parts',\n",
    "    'line',\n",
    "    'condition',\n",
    "    'conditions',\n",
    "    'section',\n",
    "    'sections',\n",
    "    'substance',\n",
    "    'substances',\n",
    "    'point',\n",
    "    'points',\n",
    "    'action',\n",
    "    'actions',\n",
    "    'statement',\n",
    "    'statements',\n",
    "    'work',\n",
    "    'works',\n",
    "    'force',\n",
    "    'forces',\n",
    "    'spot',\n",
    "    'spots',\n",
    "    'set',\n",
    "    'sets',\n",
    "    'men',\n",
    "    'man',\n",
    "    'mans',\n",
    "    'women',\n",
    "    'woman',\n",
    "    'portion',\n",
    "    'portions',\n",
    "    'piece',\n",
    "    'pieces',\n",
    "    'thought',\n",
    "    'thoughts',\n",
    "    'people',\n",
    "    'peoples',\n",
    "    'instrument',\n",
    "    'instruments',\n",
    "    'country',\n",
    "    'countries',\n",
    "    'paper',\n",
    "    'papers',\n",
    "    'information',\n",
    "    'informations',\n",
    "    'land',\n",
    "    'lands',\n",
    "    'field',\n",
    "    'fields',\n",
    "    'form',\n",
    "    'forms',\n",
    "    'situation',\n",
    "    'situations',\n",
    "    'way',\n",
    "    'ways',\n",
    "    'play',\n",
    "    'plays',\n",
    "    'parcel',\n",
    "    'parcels',\n",
    "    'expert',\n",
    "    'experts',\n",
    "]\n",
    "df2 = df2.loc[df2['hypernym'].apply(lambda x: x not in stop_hypers)]\n",
    "print(f\"Num pairs: {len(df2)}\")\n",
    "df2.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(\"hypcc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_COL = 'masked_text'\n",
    "LABEL_COL = 'masked_tokens'\n",
    "REL_COL = \"context_token\"\n",
    "\n",
    "def group_dataset(df: pd.DataFrame, rel: str, number_rel: str, number_ctx: str, prefix: str = \"\"):\n",
    "    df = df.copy()\n",
    "    assert rel in ['hypernym', 'hyponym'], \"rel must be 'hypernym' or 'hyponym'.\"\n",
    "    assert number_rel in ['singular', 'plural', 'all'], \"number_rel must be 'singular' or 'plural'.\"\n",
    "    assert number_ctx in ['singular', 'plural', 'all'], \"number_context must be 'singular' or 'plural'.\"\n",
    "    ctx = \"hypernym\" if rel == \"hyponym\" else \"hyponym\"\n",
    "    text_col = f\"masked_{rel}\"\n",
    "    label_col = rel\n",
    "    \n",
    "    df[text_col] = df[text_col].apply(lambda x: prefix + x)\n",
    "    df = df.drop(df[(df[f\"number_{rel}\"]!=number_rel) | (df[f\"number_{ctx}\"]!=number_ctx)].index)\n",
    "    df = df.drop([f\"masked_{ctx}\", f\"number_{rel}\", f\"number_{ctx}\"], axis=1)\n",
    "    df = df.drop([f\"lex_{ctx}\", f\"lex_{rel}\", f\"syn_{ctx}\", f\"syn_{rel}\"], axis=1)\n",
    "    df = df.rename({text_col: TEXT_COL}, axis=1)\n",
    "    df = df.rename({label_col: LABEL_COL}, axis=1)\n",
    "    df = df.rename({ctx: REL_COL}, axis=1)\n",
    "    agg_cols = [TEXT_COL, REL_COL]\n",
    "    df = df.groupby(agg_cols).agg(tuple).applymap(list).reset_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"hypcc.csv\")\n",
    "prefixes = dict(\n",
    "    no_ctx=\"\",\n",
    "    ctx=\"In the context of hypernymy, \",\n",
    ")\n",
    "datasets = {}\n",
    "for p, prefix in prefixes.items():\n",
    "    for rel in ['hypernym', 'hyponym']:\n",
    "        for num_rel in ['singular', 'plural']:\n",
    "            for num_ctx in ['singular', 'plural']:\n",
    "                k = \"_\".join([rel,num_rel,num_ctx,p])\n",
    "                datasets[k] = group_dataset(\n",
    "                    df=df2, \n",
    "                    rel=rel, \n",
    "                    number_rel=num_rel, \n",
    "                    number_ctx=num_ctx,\n",
    "                    prefix=prefix\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZERS = ['bert-base-uncased', 'roberta-base']\n",
    "tknzrs = {t: tr.AutoTokenizer.from_pretrained(t) for t in TOKENIZERS}\n",
    "\n",
    "tokenized = {}\n",
    "categories = {}\n",
    "for t in TOKENIZERS:\n",
    "    tknzr = tknzrs[t]\n",
    "    tokenized[t] = {}\n",
    "    categories[t] = {}\n",
    "    for k, d in datasets.items():\n",
    "        d = ds.Dataset.from_pandas(d)\n",
    "        # replace mask token with tokenizer specific token and add punctuation\n",
    "        d = d.map(lambda e: {TEXT_COL: e[TEXT_COL].replace(\"[MASK]\", tknzr.mask_token)})\n",
    "\n",
    "        # tokenize\n",
    "        d = d.map(lambda e: tknzr(e[TEXT_COL]), batched=True)\n",
    "\n",
    "        # add conservative padding\n",
    "        max_length = max([len(x) for x in d['input_ids']])\n",
    "        d = d.map(lambda e: tknzr.pad(e, max_length=max_length), batched=True)\n",
    "\n",
    "        # encode label without special tokens (add prefix space for gpt/roberta tokenizers \n",
    "        # as the target is not a start of sentence, see:\n",
    "        # https://discuss.huggingface.co/t/bpe-tokenizers-and-spaces-before-words/475/2)\n",
    "        d = d.map(lambda e: {'labels': [tknzr.encode(f' {x}', add_special_tokens=False) for x in e[LABEL_COL]]})\n",
    "        d = d.map(lambda e: {REL_COL: tknzr.encode(f' {e[REL_COL]}', add_special_tokens=False)})\n",
    "\n",
    "        # find invalid targets (i.e. multi-token words)\n",
    "        invalid_targets = [x for x in d['labels'] if any([len(_x) > 1 for _x in x])]\n",
    "        invalid_targets += [x for x in d[REL_COL] if len(x) > 1]\n",
    "        if invalid_targets:\n",
    "            invalid_targets = set([tknzr.decode(x) for x in invalid_targets])\n",
    "            raise ValueError(f\"Invalid tokenizer {t}, the following targets are multi-token: {invalid_targets}.\")\n",
    "\n",
    "        # now that we know every label is single token, remove list formatting\n",
    "        d = d.map(lambda e: {'labels': [x[0] for x in e['labels']]})\n",
    "        d = d.map(lambda e: {REL_COL: e[REL_COL][0]})\n",
    "\n",
    "        # let ds handle casting data to torch tensors, but specify output_all_columns to obtain non-tensor labels\n",
    "        d.set_format(type='torch', columns=['input_ids', 'attention_mask'], output_all_columns=True)\n",
    "\n",
    "        tokenized[t][k] = d\n",
    "        categories[t][k] = list(sorted(set(chain.from_iterable(d['labels']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized['roberta-base']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in tokenized:\n",
    "    for k in tokenized[t]:\n",
    "        tokenized[t][k].save_to_disk(f\"preprocessed/hypcc_{t}_{k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (emnlp2023)",
   "language": "python",
   "name": "emnlp2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
