{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/network/scratch/m/mirceara/.cache/huggingface/transformers\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/network/scratch/m/mirceara/.cache/huggingface/datasets\"\n",
    "os.environ[\"BALAUR_CACHE\"] = \"/network/scratch/m/mirceara/.cache/balaur\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/mila/m/mirceara/balaur/experiments/pretrain/\")\n",
    "from run_bort import MlmModel, MlmModelConfig, MlmWnreDataModule, MlmWnreDataModuleConfig, WNRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm, trange\n",
    "import json\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import datasets as ds\n",
    "import transformers as tr\n",
    "\n",
    "from typing import List, Union\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_COL = 'masked_text'\n",
    "LABEL_COL = 'masked_tokens'\n",
    "REL_COL = \"context_token\"\n",
    "\n",
    "tokenized = {}\n",
    "tokenizers = ['roberta-base', 'bert-base-uncased']\n",
    "setting = 'ctx'\n",
    "tokenized_categories = {}\n",
    "for t in tokenizers:\n",
    "    tokenized[t] = {}\n",
    "    tokenized_categories[t] = {}\n",
    "    for task in ['hypernym', 'hyponym']:\n",
    "        d = ds.load_from_disk(f\"preprocessed/hypcc_{t}_{task}_singular_singular_{setting}\")\n",
    "        tokenized_categories[t][task] = list(sorted(set(chain.from_iterable(d['labels']))))\n",
    "        tokenized[t][task] = d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(ckpt_dir: Path, step: int, **config_kwargs):\n",
    "    # get checkpoint\n",
    "    p = list(ckpt_dir.glob(f\"*step={step}.ckpt\"))\n",
    "    assert len(p) == 1, f\"Unresolvable paths: {p}\"\n",
    "    p = p[0]\n",
    "\n",
    "    # load model and config\n",
    "    # HACK: load model by reconstructing config from ckpt\n",
    "    #       for quick and dirty evaluation of trained models.\n",
    "    ckpt = torch.load(p)\n",
    "    config = MlmModelConfig()\n",
    "    config.__dict__.update(ckpt['hyper_parameters'])\n",
    "    config.__dict__.update(config_kwargs)\n",
    "    model = MlmModel(config=config)\n",
    "    model.load_state_dict(ckpt['state_dict'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HypccMetrics:\n",
    "    def __init__(self):\n",
    "        self.open_acc1 = []\n",
    "        self.open_acc5 = []\n",
    "        self.open_rank = []\n",
    "        self.closed_acc1 = []\n",
    "        self.closed_acc5 = []\n",
    "        self.closed_rank = []\n",
    "        self.top10_labels = []\n",
    "        self.top10_probs = []\n",
    "        self.nll = []\n",
    "        self.labels = []\n",
    "        self.prompts = []\n",
    "        self.rep_prob = []\n",
    "\n",
    "    def update(self,\n",
    "               logits: torch.Tensor,    # (bsz,vsz)\n",
    "               related: List[int],      # (bsz,)\n",
    "               labels: List[List[int]], # (bsz, nlabels), nlabels varies within a batch\n",
    "               closed_vocab: List[int],\n",
    "               ):\n",
    "        \n",
    "        nll_tensor = -F.log_softmax(logits, dim=-1)\n",
    "        nlls = []\n",
    "        closed_ranks = []\n",
    "        open_ranks = []\n",
    "        for bidx, vidxs in enumerate(labels):\n",
    "            closed_ranks.append([])\n",
    "            open_ranks.append([])\n",
    "            nlls.append([])\n",
    "            rank_scores = logits[bidx].clone()\n",
    "            rank_scores[vidxs] = float('-inf')\n",
    "            for vidx in vidxs:\n",
    "                nlls[-1].append(nll_tensor[bidx][vidx].item())\n",
    "                label_gt = logits[bidx][vidx] < rank_scores\n",
    "                open_ranks[-1].append(int(label_gt.sum() + 1))\n",
    "                closed_ranks[-1].append(int(label_gt[closed_vocab].sum() + 1))\n",
    "        \n",
    "        probs = torch.exp(-nll_tensor)\n",
    "        top10 = probs.topk(10, dim=1)\n",
    "        self.top10_probs += top10[0].tolist()\n",
    "        self.top10_labels += top10[1].tolist()\n",
    "        for bidx, vidx in enumerate(related):\n",
    "            self.rep_prob.append(probs[bidx][vidx].item())\n",
    "        \n",
    "        \n",
    "        self.nll += nlls\n",
    "        self.closed_rank += closed_ranks\n",
    "        self.open_rank += open_ranks\n",
    "        self.closed_acc1 += [[int(r == 1) for r in rs] for rs in closed_ranks]\n",
    "        self.closed_acc5 += [[int(r <= 5) for r in rs] for rs in closed_ranks]\n",
    "        self.open_acc1 += [[int(r == 1) for r in rs] for rs in open_ranks]\n",
    "        self.open_acc5 += [[int(r <= 5) for r in rs] for rs in open_ranks]\n",
    "        self.labels += labels\n",
    "        self.prompts += related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_collate(batch):\n",
    "    tensor_batch = [{k: x[k] for k in ['input_ids', 'attention_mask']} for x in batch]\n",
    "    out = torch.utils.data.dataloader.default_collate(tensor_batch)\n",
    "    out['labels'] = [x['labels'] for x in batch]\n",
    "    out[REL_COL] = [x[REL_COL] for x in batch]\n",
    "    return out\n",
    "\n",
    "def eval_loop(*, \n",
    "              model_name: str,\n",
    "              model: Union[MlmModel, tr.AutoModel],\n",
    "              tokenizer: tr.AutoTokenizer, \n",
    "              dataset: ds.Dataset, \n",
    "              categories: List[int], \n",
    "              bsz: int,\n",
    "              device: torch.device = None):\n",
    "    # setup\n",
    "    device = device or torch.device('cpu')\n",
    "    model.to(device)\n",
    "    dl = torch.utils.data.DataLoader(dataset, batch_size=bsz, shuffle=False, drop_last=False, collate_fn=labels_collate)\n",
    "    metrics = HypccMetrics()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(dl, total=len(dl)):\n",
    "            labels = batch.pop('labels')\n",
    "            related = batch.pop(REL_COL)\n",
    "            batch = {k:v.to(device) for k,v in batch.items()}\n",
    "            mask_idx = batch['input_ids'] == tokenizer.mask_token_id\n",
    "            hidden_states = model(batch)\n",
    "            embeds = hidden_states[mask_idx]\n",
    "            logits = model.head(embeds)\n",
    "            metrics.update(\n",
    "                logits,\n",
    "                related,\n",
    "                labels,\n",
    "                categories\n",
    "            )\n",
    "    model.to(torch.device('cpu'))\n",
    "    return metrics \n",
    "\n",
    "def hf_bert_eval_loop(*, \n",
    "              model_name: str,\n",
    "              model: tr.AutoModel,\n",
    "              tokenizer: tr.AutoTokenizer, \n",
    "              dataset: ds.Dataset, \n",
    "              categories: List[int], \n",
    "              bsz: int,\n",
    "              device: torch.device = None):\n",
    "    # setup\n",
    "    device = device or torch.device('cpu')\n",
    "    model.to(device)\n",
    "    dl = torch.utils.data.DataLoader(dataset, batch_size=bsz, shuffle=False, drop_last=False, collate_fn=labels_collate)\n",
    "    metrics = HypccMetrics()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(dl, total=len(dl)):\n",
    "            labels = batch.pop('labels')\n",
    "            related = batch.pop(REL_COL)\n",
    "            batch = {k:v.to(device) for k,v in batch.items()}\n",
    "            mask_idx = batch['input_ids'] == tokenizer.mask_token_id\n",
    "            out = model(**batch, output_hidden_states=False)        \n",
    "            metrics.update(\n",
    "                out.logits[mask_idx],\n",
    "                related,\n",
    "                labels,\n",
    "                categories\n",
    "            )\n",
    "    model.to(torch.device('cpu'))\n",
    "    return metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 512\n",
    "step = 25000\n",
    "device = torch.device('cuda')\n",
    "\n",
    "get_ckpt_dir = lambda m: Path(f\"/home/mila/m/mirceara/scratch/.cache/balaur/runs/{m}/balaur/{m}/checkpoints/\")\n",
    "\n",
    "models = [\n",
    "    (\"mlm_wnre\", \"roberta-base\", eval_loop),\n",
    "    (\"mlm_only\", \"roberta-base\", eval_loop),\n",
    "#     (\"roberta-base\", \"roberta-base\", hf_bert_eval_loop),\n",
    "#     (\"roberta-large\", \"roberta-base\", hf_bert_eval_loop),\n",
    "#     (\"bert-base-uncased\", \"bert-base-uncased\", hf_bert_eval_loop),\n",
    "    (\"bert-large-uncased\", \"bert-base-uncased\", hf_bert_eval_loop),\n",
    "\n",
    "]\n",
    "TOKENIZERS = ['bert-base-uncased', 'roberta-base']\n",
    "tknzrs = {t: tr.AutoTokenizer.from_pretrained(t) for t in TOKENIZERS}\n",
    "\n",
    "\n",
    "metrics = {}\n",
    "for m, t, eval_fn in models:\n",
    "    metrics[m] = {}\n",
    "    if eval_fn == hf_bert_eval_loop:\n",
    "        model = tr.AutoModelForMaskedLM.from_pretrained(m)\n",
    "    else:\n",
    "        model = load_model(get_ckpt_dir(m), step)\n",
    "    print(m)\n",
    "    for task, dataset in tokenized[t].items():\n",
    "        with torch.amp.autocast(\"cuda\"):\n",
    "            metrics[m][task] = eval_fn(\n",
    "                model_name=m,\n",
    "                model=model,\n",
    "                tokenizer=tknzrs[t],\n",
    "                dataset=dataset,\n",
    "                categories=tokenized_categories[t][task],\n",
    "                bsz=bsz,\n",
    "                device=device,\n",
    "            )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = {}\n",
    "for task in ['hypernym', 'hyponym']:\n",
    "    df_rows = {}\n",
    "    for model in metrics.keys():\n",
    "        _metrics = metrics[model][task]\n",
    "        closed_mrr = [np.mean([1/x for x in xx]) for xx in _metrics.closed_rank]\n",
    "        open_mrr = [np.mean([1/x for x in xx]) for xx in _metrics.open_rank]\n",
    "        nll = [np.mean(xx) for xx in _metrics.nll]\n",
    "        prob = [np.sum(np.exp(np.negative(xx))) for xx in _metrics.nll]\n",
    "        col = dict(\n",
    "            closed_mrr = np.mean(closed_mrr),\n",
    "            open_mrr = np.mean(open_mrr),\n",
    "            closed_acc1 = np.mean([np.mean(xx) for xx in _metrics.closed_acc1]),\n",
    "            closed_acc5 = np.mean([np.mean(xx) for xx in _metrics.closed_acc5]),\n",
    "            open_acc1 = np.mean([np.mean(xx) for xx in _metrics.open_acc1]),\n",
    "            open_acc5 = np.mean([np.mean(xx) for xx in _metrics.open_acc5]),\n",
    "            nll = np.mean(nll),\n",
    "            prob = np.mean(prob),\n",
    "        )\n",
    "        # formatting\n",
    "        for k in col.keys():\n",
    "            if any([x in k for x in ['acc', 'prob', 'mrr']]):\n",
    "                col[k] *= 100\n",
    "            col[k] = f\"{col[k]:.3f}\"\n",
    "        df_rows[model] = col\n",
    "    results_df[task] = pd.DataFrame.from_dict(df_rows)\n",
    "    \n",
    "for k, df in results_df.items():\n",
    "    display(k)\n",
    "    display(df.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Repetition Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repetitions(_metrics: HypccMetrics, m: str, t: str, task: str = 'hypernym'):\n",
    "    preds = [x[0] for x in _metrics.top10_labels]\n",
    "    ctxts = _metrics.prompts\n",
    "    is_repeat = [ctxt==pred for ctxt, pred in zip(ctxts, preds)]\n",
    "    return is_repeat\n",
    "\n",
    "def get_repetition_rate(*args, **kwargs):\n",
    "    is_repeat = get_repetitions(*args, **kwargs)\n",
    "    return sum(is_repeat) / len(is_repeat)\n",
    "\n",
    "for task in ['hypernym', 'hyponym']:\n",
    "    repetition_df = pd.DataFrame()\n",
    "    for m,t,_ in models:\n",
    "        repetition_df[m] = [f\"{(100*get_repetition_rate(metrics[m][task], m, t, task)):.2f}%\"]\n",
    "    print(task)\n",
    "    display(repetition_df.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_token(token_id: int, m: str):\n",
    "    if m.startswith('roberta') or m.startswith('mlm'):\n",
    "        token = tknzrs['roberta-base'].convert_ids_to_tokens(token_id)\n",
    "        if token[0] == 'Ġ':\n",
    "            return token[1:]\n",
    "        else:\n",
    "            return 'Ġ'+token\n",
    "    elif m.startswith('bert'):\n",
    "        token = tknzrs['bert-base-uncased'].convert_ids_to_tokens(token_id)\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'hypernym'\n",
    "context_word = 'church'\n",
    "\n",
    "m = 'mlm_wnre'\n",
    "t = 'roberta-base'\n",
    "print(m)\n",
    "for i,preds in enumerate(metrics[m][task].top10_labels):\n",
    "    if convert_token(metrics[m][task].prompts[i], t) != context_word:\n",
    "        continue\n",
    "    x = tokenized[t][task]['masked_text'][i]\n",
    "    probs = metrics[m][task].top10_probs[i]\n",
    "    toks = [convert_token(p, m) for p in preds]\n",
    "    tok_probs = [f\"{t} ({100*p:.2f})\" for  t,p in zip(toks, probs)]\n",
    "    print(x)\n",
    "    print(\"\\\\textsc{Bert}\\t\\t&\\t\" + \" &\\t\".join(toks[:5]) + \" \\\\\\\\\")\n",
    "    print(\"\\\\textsc{+Balaur}\\t&\\t\" + \" &\\t\".join([f\"${100*p:.2f}$\" for  p in probs][:5]) + \" \\\\\\\\\")\n",
    "    \n",
    "    \n",
    "m = 'mlm_only'\n",
    "t = 'roberta-base'\n",
    "print(\"\\n\")\n",
    "print(m)\n",
    "for i,preds in enumerate(metrics[m][task].top10_labels):\n",
    "    if convert_token(metrics[m][task].prompts[i], t) != context_word:\n",
    "        continue\n",
    "    x = tokenized[t][task]['masked_text'][i]\n",
    "    probs = metrics[m][task].top10_probs[i]\n",
    "    toks = [convert_token(p, m) for p in preds]\n",
    "    tok_probs = [f\"{t} ({100*p:.2f})\" for  t,p in zip(toks, probs)]\n",
    "    print(x)\n",
    "    print(\"\\\\textsc{Bert}\\t\\t&\\t\" + \" &\\t\".join(toks[:5]) + \" \\\\\\\\\")\n",
    "    print(\"\\t\\t\\t&\\t\" + \" &\\t\".join([f\"${100*p:.2f}$\" for  p in probs][:5]) + \" \\\\\\\\\")\n",
    "    \n",
    "    labels =  metrics[m][task].labels[i]\n",
    "    labels = [convert_token(l, m) for l in labels]\n",
    "\n",
    "print()\n",
    "print(\"labels\")\n",
    "print(\", \".join(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (blr)",
   "language": "python",
   "name": "blr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
