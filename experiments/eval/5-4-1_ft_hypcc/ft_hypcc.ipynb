{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/network/scratch/m/mirceara/.cache/huggingface/transformers\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/network/scratch/m/mirceara/.cache/huggingface/datasets\"\n",
    "os.environ[\"BALAUR_CACHE\"] = \"/network/scratch/m/mirceara/.cache/balaur\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/mila/m/mirceara/balaur/experiments/pretrain/\")\n",
    "from run_bort import MlmModel, MlmModelConfig, MlmWnreDataModule, MlmWnreDataModuleConfig, WNRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "from itertools import chain\n",
    "import copy\n",
    "from functools import lru_cache\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import transformers as tr\n",
    "import datasets as ds\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from balaur import ROOT_DIR\n",
    "\n",
    "from typing import List\n",
    "\n",
    "exp_dir = Path(\"/network/scratch/m/mirceara/.cache/balaur/runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_COL = 'masked_text'\n",
    "LABEL_COL = 'masked_tokens'\n",
    "REL_COL = \"context_token\"\n",
    "\n",
    "tokenizer = 'roberta-base'\n",
    "setting = 'ctx'\n",
    "tokenized = {}\n",
    "tokenized_categories = {}\n",
    "for task in ['hypernym', 'hyponym']:\n",
    "    d = ds.load_from_disk(f\"../5-2_hypcc/preprocessed/hypcc_{tokenizer}_{task}_singular_singular_{setting}\")\n",
    "    tokenized_categories[task] = list(sorted(set(chain.from_iterable(d['labels']))))\n",
    "    tokenized[task] = d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(ckpt_dir: Path, step: int, **config_kwargs):\n",
    "    # get checkpoint\n",
    "    p = list(ckpt_dir.glob(f\"*step={step}.ckpt\"))\n",
    "    assert len(p) == 1, f\"Unresolvable paths: {p}\"\n",
    "    p = p[0]\n",
    "\n",
    "    # load model and config\n",
    "    # HACK: load model by reconstructing config from ckpt\n",
    "    #       for quick and dirty evaluation of trained models.\n",
    "    ckpt = torch.load(p)\n",
    "    config = MlmModelConfig()\n",
    "    config.__dict__.update(ckpt['hyper_parameters'])\n",
    "    config.__dict__.update(config_kwargs)\n",
    "    model = MlmModel(config=config)\n",
    "    model.load_state_dict(ckpt['state_dict'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model, unfreeze_str: str = \"head\"):\n",
    "    for n,p in model.named_parameters():\n",
    "        if n.split(\".\")[0] == 'head':\n",
    "            p.requires_grad = True\n",
    "            print(f\"Unfreezing: {n}\")\n",
    "        else:\n",
    "            p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HypccMetrics:\n",
    "    def __init__(self):\n",
    "        self.open_acc1 = []\n",
    "        self.open_acc5 = []\n",
    "        self.open_rank = []\n",
    "        self.closed_acc1 = []\n",
    "        self.closed_acc5 = []\n",
    "        self.closed_rank = []\n",
    "        self.top10_labels = []\n",
    "        self.top10_probs = []\n",
    "        self.nll = []\n",
    "        self.labels = []\n",
    "        self.prompts = []\n",
    "        self.rep_prob = []\n",
    "\n",
    "    def update(self,\n",
    "               logits: torch.Tensor,    # (bsz,vsz)\n",
    "               related: List[int],      # (bsz,)\n",
    "               labels: List[List[int]], # (bsz, nlabels), nlabels varies within a batch\n",
    "               closed_vocab: List[int],\n",
    "               ):\n",
    "        \n",
    "        nll_tensor = -F.log_softmax(logits, dim=-1)\n",
    "        nlls = []\n",
    "        closed_ranks = []\n",
    "        open_ranks = []\n",
    "        for bidx, vidxs in enumerate(labels):\n",
    "            closed_ranks.append([])\n",
    "            open_ranks.append([])\n",
    "            nlls.append([])\n",
    "            rank_scores = logits[bidx].clone()\n",
    "            rank_scores[vidxs] = float('-inf')\n",
    "            for vidx in vidxs:\n",
    "                nlls[-1].append(nll_tensor[bidx][vidx].item())\n",
    "                label_gt = logits[bidx][vidx] < rank_scores\n",
    "                open_ranks[-1].append(int(label_gt.sum() + 1))\n",
    "                closed_ranks[-1].append(int(label_gt[closed_vocab].sum() + 1))\n",
    "        \n",
    "        probs = torch.exp(-nll_tensor)\n",
    "        top10 = probs.topk(10, dim=1)\n",
    "        self.top10_probs += top10[0].tolist()\n",
    "        self.top10_labels += top10[1].tolist()\n",
    "        for bidx, vidx in enumerate(related):\n",
    "            self.rep_prob.append(probs[bidx][vidx].item())\n",
    "        \n",
    "        \n",
    "        self.nll += nlls\n",
    "        self.closed_rank += closed_ranks\n",
    "        self.open_rank += open_ranks\n",
    "        self.closed_acc1 += [[int(r == 1) for r in rs] for rs in closed_ranks]\n",
    "        self.closed_acc5 += [[int(r <= 5) for r in rs] for rs in closed_ranks]\n",
    "        self.open_acc1 += [[int(r == 1) for r in rs] for rs in open_ranks]\n",
    "        self.open_acc5 += [[int(r <= 5) for r in rs] for rs in open_ranks]\n",
    "        self.labels += labels\n",
    "        self.prompts += related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_collate(batch):\n",
    "    tensor_batch = [{k: x[k] for k in ['input_ids', 'attention_mask']} for x in batch]\n",
    "    out = torch.utils.data.dataloader.default_collate(tensor_batch)\n",
    "    out[REL_COL] = [x[REL_COL] for x in batch]\n",
    "    out['labels'] = [x['labels'] for x in batch]\n",
    "    return out\n",
    "\n",
    "def eval_loop(*, \n",
    "              model: MlmModel,\n",
    "              tokenizer: tr.AutoTokenizer, \n",
    "              dataset: ds.Dataset, \n",
    "              categories: List[int], \n",
    "              bsz: int,\n",
    "              device: torch.device = None,\n",
    "             ):\n",
    "    # setup\n",
    "    device = device or torch.device('cpu')\n",
    "    model.to(device)\n",
    "    dl = torch.utils.data.DataLoader(dataset, batch_size=bsz, shuffle=False, drop_last=False, collate_fn=labels_collate)\n",
    "    metrics = HypccMetrics()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch in tqdm(dl, total=len(dl)):\n",
    "            labels = batch.pop('labels')\n",
    "            related = batch.pop(REL_COL)\n",
    "            batch = {k:v.to(device) for k,v in batch.items()}\n",
    "            mask_idx = batch['input_ids'] == tokenizer.mask_token_id\n",
    "            hidden_states = model(batch)\n",
    "            embeds = hidden_states[mask_idx]\n",
    "            logits = model.head(embeds)\n",
    "\n",
    "            metrics.update(\n",
    "                logits.detach().cpu().float(),\n",
    "                related,\n",
    "                labels,\n",
    "                categories\n",
    "            )\n",
    "    return metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### oLMpics Finetune Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_multilabel(examples):\n",
    "    unpacked = {k: [] for k in examples}\n",
    "    for i in range(len(examples['input_ids'])):\n",
    "        example = {k: v[i] for k,v in examples.items()}\n",
    "        labels = example.pop('labels')\n",
    "        masked_tokens = example.pop('masked_tokens')\n",
    "        for l,m in zip(labels, masked_tokens):\n",
    "            for k,v in example.items():\n",
    "                unpacked[k].append(v)\n",
    "            unpacked['labels'].append(l)\n",
    "            unpacked['masked_tokens'].append(m)\n",
    "            \n",
    "    return unpacked\n",
    "\n",
    "def get_train_eval_sets(task: str, seed: int, split=0.8):\n",
    "    ds_dict = tokenized[task].train_test_split(seed=seed, train_size=split)\n",
    "    \n",
    "    train_ds = ds_dict['train']    \n",
    "    train_ds = train_ds.map(unpack_multilabel, batched=True)\n",
    "    train_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'], output_all_columns=True)\n",
    "    \n",
    "    eval_ds = ds_dict['test']\n",
    "    eval_ds.set_format(type='torch', columns=['input_ids', 'attention_mask'], output_all_columns=True)\n",
    "\n",
    "    return train_ds, eval_ds\n",
    "\n",
    "def ft_labels_collate(batch):\n",
    "    tensor_batch = [{k: x[k] for k in ['input_ids', 'attention_mask', 'labels']} for x in batch]\n",
    "    out = torch.utils.data.dataloader.default_collate(tensor_batch)\n",
    "    return out\n",
    "    \n",
    "def finetune_loop(*, \n",
    "                  model: MlmModel, \n",
    "                  tokenizer: tr.AutoTokenizer,\n",
    "                  task: str = 'hypernym',\n",
    "                  bsz: int = 16, \n",
    "                  lr: float = 4e-4, \n",
    "                  wd: float = 0.1,\n",
    "                  device: torch.device = None,\n",
    "                  num_seeds: int = 5, start_seed: int = 1337,\n",
    "                  warmup_ratio: float = 0.06,\n",
    "                  betas: list[float] = [0.9, 0.98],\n",
    "                  max_steps: int = 4096,\n",
    "                  split: float = 0.8,\n",
    "                  freeze: bool = True,\n",
    "                  schedule: str = 'linear',\n",
    "                 ):\n",
    "    \n",
    "    device = device or torch.device('cpu')\n",
    "    metrics = {}\n",
    "    losses = {}\n",
    "    seed = start_seed\n",
    "    for seed_incr in range(num_seeds):\n",
    "        seed_model = copy.deepcopy(model)\n",
    "        seed_model.train()\n",
    "        if freeze:\n",
    "            freeze_model(seed_model)\n",
    "        seed_model = seed_model.to(device)\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in seed_model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": wd,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in seed_model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=lr, weight_decay=wd, betas=betas)\n",
    "        lr_scheduler = tr.get_scheduler(schedule, optimizer, int(max_steps * warmup_ratio), max_steps)\n",
    "        \n",
    "        seed = seed + seed_incr\n",
    "        train_ds, eval_ds = get_train_eval_sets(task, seed, split)\n",
    "        categories = tokenized_categories[task]\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed)\n",
    "        dl = torch.utils.data.DataLoader(\n",
    "            train_ds, batch_size=bsz, shuffle=True, drop_last=False, \n",
    "            generator=g, collate_fn=ft_labels_collate,\n",
    "        )\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        metrics[seed] = {}\n",
    "        losses[seed] = {}\n",
    "        steps = 0\n",
    "        next_eval_step = 0\n",
    "        while steps < max_steps:\n",
    "            print(\"new epoch\")\n",
    "            for batch in dl:\n",
    "                batch = {k:v.to(device) for k,v in batch.items()}\n",
    "                optimizer.zero_grad()\n",
    "                # begin mlm loss computation in original model\n",
    "                src = seed_model(batch)\n",
    "                src = src.view(-1, src.shape[-1])\n",
    "                mask_unmasked = batch['input_ids'].view(-1) == seed_model.bort_config.mask_token_id\n",
    "                mlm_src = src[mask_unmasked]\n",
    "                logits = seed_model.head(mlm_src)\n",
    "                loss = F.cross_entropy(logits, batch['labels'])\n",
    "                # end mlm loss computation\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                loss = loss.item()\n",
    "                if steps == next_eval_step:\n",
    "                    if steps == 0:\n",
    "                        next_eval_step = bsz\n",
    "                    else:\n",
    "                        next_eval_step *= 2\n",
    "                    \n",
    "                    metrics[seed][steps] = eval_loop(\n",
    "                        model=seed_model,\n",
    "                        tokenizer=tokenizer, \n",
    "                        dataset=eval_ds,\n",
    "                        categories=categories,\n",
    "                        bsz=1024, \n",
    "                        device=device,\n",
    "                    )\n",
    "                    losses[seed][steps] = loss\n",
    "\n",
    "                if steps >= max_steps:\n",
    "                    break \n",
    "                    \n",
    "                steps += bsz\n",
    "                \n",
    "    return metrics,losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAMES = [\"mlm_only\", \"mlm_wnre\"]\n",
    "STEP = 25_000\n",
    "\n",
    "models = {\n",
    "    m: load_model(Path(f\"{exp_dir}/{m}/balaur/{m}/checkpoints/\"), STEP)\n",
    "    for m in MODEL_NAMES\n",
    "}\n",
    "tknzr = tr.AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP = True # this takes quite some time, set to False to run.\n",
    "\n",
    "if not SKIP:\n",
    "    eval_metrics = defaultdict(dict)\n",
    "    train_losses = defaultdict(dict)\n",
    "    for m, model in models.items():\n",
    "        for task in ['hypernym']:\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                eval_metrics[task][m], train_losses[task][m] = finetune_loop(\n",
    "                    model=model, \n",
    "                    tokenizer=tknzr, \n",
    "                    task=task,\n",
    "                    bsz=16, \n",
    "                    lr=4e-4,\n",
    "                    device=torch.device('cuda'),\n",
    "                    num_seeds=5,\n",
    "                    max_steps=2048*4*8,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP:\n",
    "    with open(\"ft_results.pkl\", \"wb\") as f:\n",
    "        pickle.dump(eval_metrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SKIP:\n",
    "    with open(\"ft_results.pkl\", \"rb\") as f:\n",
    "        eval_metrics = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "from statistics import quantiles\n",
    "\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trace(x, y, legend: str, rgb='0,100,80', dash=None, mode='lines', z=None):\n",
    "    trace = [\n",
    "        go.Scatter(\n",
    "            name=legend,\n",
    "            x=x,\n",
    "            y=y,\n",
    "            hovertext=z,\n",
    "            line=dict(color=f'rgb({rgb})', dash=dash),\n",
    "            mode=mode\n",
    "        ),\n",
    "    ]\n",
    "    return trace\n",
    "\n",
    "def plot_mrr(task: str, model1: str, model2: str, leg1: str, leg2: str, rgb1: str='45,114,178', rgb2: str='194,24,7'):\n",
    "    traces = []\n",
    "    for obj, leg, rgb in zip([model1, model2], [leg1, leg2], [rgb1,rgb2]):\n",
    "        x = []\n",
    "        y = []\n",
    "        mrrs = defaultdict(list)\n",
    "        for seed in eval_metrics[task][obj]:\n",
    "            for step, metric in eval_metrics[task][obj][seed].items():\n",
    "                mrr = [np.mean([1/x for x in xx]) for xx in metric.open_rank]\n",
    "                mrrs[step].extend(mrr)\n",
    "        for step in sorted(eval_metrics[task][obj][seed]):\n",
    "            mrr = np.mean([np.mean(xx) for xx in mrrs[step]])\n",
    "            y.append(mrr)          \n",
    "            x.append(step)\n",
    "        traces.extend(plot_trace(x,y,leg,rgb))\n",
    "    fig = go.Figure(traces)\n",
    "    return fig\n",
    "\n",
    "def plot_rep(task: str, model1: str, model2: str, leg1: str, leg2: str, rgb1: str='45,114,178', rgb2: str='194,24,7'):\n",
    "    traces = []\n",
    "    for obj, leg, rgb in zip([model1, model2], [leg1, leg2], [rgb1,rgb2]):\n",
    "        x = []\n",
    "        y = []\n",
    "        reps = defaultdict(list)\n",
    "        for seed in eval_metrics[task][obj]:\n",
    "            for step, metric in eval_metrics[task][obj][seed].items():\n",
    "                rep = [int(pred[0] == ctxt) for pred, ctxt in zip(metric.top10_labels, metric.prompts)]\n",
    "                reps[step].extend(rep)\n",
    "        for step in sorted(eval_metrics[task][obj][seed]):\n",
    "            rep = np.mean([np.mean(xx) for xx in reps[step]])\n",
    "            y.append(rep)          \n",
    "            x.append(step)\n",
    "        traces.extend(plot_trace(x,y,leg,rgb))\n",
    "    fig = go.Figure(traces)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LEGENDS = {\n",
    "    \"mlm_only\": \"BERT (OURS)\",\n",
    "    \"mlm_wnre\": \"BERT+BALAUR\"\n",
    "}\n",
    "MODEL_RGBS = {\n",
    "    \"mlm_only\": \"255,127,80\",\n",
    "    \"mlm_wnre\": \"65,105,225\"\n",
    "}\n",
    "\n",
    "m2 = \"mlm_only\"\n",
    "m1 = \"mlm_wnre\"\n",
    "fig = plot_mrr(\n",
    "    'hypernym',\n",
    "    m1,m2,\n",
    "    MODEL_LEGENDS[m1], MODEL_LEGENDS[m2],\n",
    "    MODEL_RGBS[m1], MODEL_RGBS[m2]\n",
    ")\n",
    "fig.update_yaxes(type=\"linear\")\n",
    "fig.update_xaxes(type=\"linear\")\n",
    "\n",
    "fig.update_layout(showlegend=True, template='simple_white')\n",
    "fig.update_layout(legend=dict(yanchor='bottom', xanchor='right', x=0.99,y=0.01))\n",
    "\n",
    "\n",
    "fig.update_layout(width=500, height=250, \n",
    "                  font_family=\"Serif\", \n",
    "                  font_size=12, \n",
    "                  margin_l=5, margin_t=5, margin_b=5, margin_r=5)\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Number of training examples\",\n",
    "    yaxis_title=\"MRR (Hypernym)\",\n",
    ")\n",
    "display(HTML(fig.to_html()))\n",
    "pio.write_image(fig, \"ft_hypcc_openmrr_hypernym.pdf\", width=1.5*300, height=0.75*300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LEGENDS = {\n",
    "    \"mlm_only\": \"BERT (OURS)\",\n",
    "    \"mlm_wnre\": \"BERT+BALAUR\"\n",
    "}\n",
    "MODEL_RGBS = {\n",
    "    \"mlm_only\": \"255,127,80\",\n",
    "    \"mlm_wnre\": \"65,105,225\"\n",
    "}\n",
    "\n",
    "m2 = \"mlm_only\"\n",
    "m1 = \"mlm_wnre\"\n",
    "fig = plot_mrr(\n",
    "    'hyponym',\n",
    "    m1,m2,\n",
    "    MODEL_LEGENDS[m1], MODEL_LEGENDS[m2],\n",
    "    MODEL_RGBS[m1], MODEL_RGBS[m2]\n",
    ")\n",
    "fig.update_yaxes(type=\"linear\")\n",
    "fig.update_xaxes(type=\"linear\")\n",
    "\n",
    "fig.update_layout(showlegend=True, template='simple_white')\n",
    "fig.update_layout(legend=dict(yanchor='bottom', xanchor='right', x=0.99,y=0.01))\n",
    "\n",
    "\n",
    "fig.update_layout(width =500, height=250, \n",
    "                  font_family=\"Serif\", \n",
    "                  font_size=12, \n",
    "                  margin_l=5, margin_t=5, margin_b=5, margin_r=5)\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Number of training examples\",\n",
    "    yaxis_title=\"MRR (Hyponym)\",\n",
    ")\n",
    "display(HTML(fig.to_html()))\n",
    "pio.write_image(fig, \"ft_hypcc_openmrr_hyponym.pdf\", width=1.5*300, height=0.75*300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LEGENDS = {\n",
    "    \"mlm_only\": \"BERT (OURS)\",\n",
    "    \"mlm_wnre\": \"BERT+BALAUR\"\n",
    "}\n",
    "MODEL_RGBS = {\n",
    "    \"mlm_only\": \"255,127,80\",\n",
    "    \"mlm_wnre\": \"65,105,225\"\n",
    "}\n",
    "\n",
    "m2 = \"mlm_only\"\n",
    "m1 = \"mlm_wnre\"\n",
    "fig = plot_rep(\n",
    "    'hypernym',\n",
    "    m1,m2,\n",
    "    MODEL_LEGENDS[m1], MODEL_LEGENDS[m2],\n",
    "    MODEL_RGBS[m1], MODEL_RGBS[m2]\n",
    ")\n",
    "fig.update_yaxes(type=\"linear\")\n",
    "fig.update_xaxes(type=\"linear\")\n",
    "\n",
    "fig.update_layout(showlegend=True, template='simple_white')\n",
    "fig.update_layout(legend=dict(yanchor='bottom', xanchor='right', x=0.99,y=0.80))\n",
    "\n",
    "\n",
    "fig.update_layout(width=500, height=250, \n",
    "                  font_family=\"Serif\", \n",
    "                  font_size=12, \n",
    "                  margin_l=5, margin_t=5, margin_b=5, margin_r=5)\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Number of training examples\",\n",
    "    yaxis_title=\"Hypernym Repetition\",\n",
    ")\n",
    "display(HTML(fig.to_html()))\n",
    "pio.write_image(fig, \"ft_hypcc_repetition_hypernym.pdf\", width=1.5*300, height=0.75*300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LEGENDS = {\n",
    "    \"mlm_only\": \"BERT (OURS)\",\n",
    "    \"mlm_wnre\": \"BERT+BALAUR\"\n",
    "}\n",
    "MODEL_RGBS = {\n",
    "    \"mlm_only\": \"255,127,80\",\n",
    "    \"mlm_wnre\": \"65,105,225\"\n",
    "}\n",
    "\n",
    "m2 = \"mlm_only\"\n",
    "m1 = \"mlm_wnre\"\n",
    "fig = plot_rep(\n",
    "    'hyponym',\n",
    "    m1,m2,\n",
    "    MODEL_LEGENDS[m1], MODEL_LEGENDS[m2],\n",
    "    MODEL_RGBS[m1], MODEL_RGBS[m2]\n",
    ")\n",
    "fig.update_yaxes(type=\"linear\")\n",
    "fig.update_xaxes(type=\"linear\")\n",
    "\n",
    "fig.update_layout(showlegend=True, template='simple_white')\n",
    "fig.update_layout(legend=dict(yanchor='bottom', xanchor='right', x=0.99,y=0.80))\n",
    "\n",
    "\n",
    "fig.update_layout(width=500, height=250, \n",
    "                  font_family=\"Serif\", \n",
    "                  font_size=12, \n",
    "                  margin_l=5, margin_t=5, margin_b=5, margin_r=5)\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Number of training examples\",\n",
    "    yaxis_title=\"Hyponym Repetition\",\n",
    ")\n",
    "display(HTML(fig.to_html()))\n",
    "pio.write_image(fig, \"ft_hypcc_repetition_hyponym.pdf\", width=1.5*300, height=0.75*300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_intruding_preds(task: str, model1: str, model2: str, leg1: str, leg2: str, rgb1: str='45,114,178', rgb2: str='194,24,7'):\n",
    "    traces = []\n",
    "    for obj, leg, rgb in zip([model1, model2], [leg1, leg2], [rgb1,rgb2]):\n",
    "        x = []\n",
    "        y = []\n",
    "        intruder_rates = defaultdict(list)\n",
    "        for seed in eval_metrics[task][obj]:\n",
    "            for step, metric in eval_metrics[task][obj][seed].items():\n",
    "                num_preds = 0\n",
    "                num_intrd = 0\n",
    "                for i, preds in enumerate(metric.top10_labels):\n",
    "                    for rank, pred in enumerate(preds):\n",
    "                        if pred not in metric.labels[i] and pred in tokenized_categories[task]:\n",
    "                            num_intrd += 1\n",
    "                        num_preds += 1\n",
    "                intruder_rates[step].append(num_intrd / num_preds)\n",
    "        for step in sorted(eval_metrics[task][obj][seed]):\n",
    "            intruder_rate = np.mean(intruder_rates[step])\n",
    "            y.append(intruder_rate)          \n",
    "            x.append(step)\n",
    "        traces.extend(plot_trace(x,y,leg,rgb))\n",
    "    fig = go.Figure(traces)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LEGENDS = {\n",
    "    \"mlm_only\": \"BERT (OURS)\",\n",
    "    \"mlm_wnre\": \"BERT+BALAUR\"\n",
    "}\n",
    "MODEL_RGBS = {\n",
    "    \"mlm_only\": \"255,127,80\",\n",
    "    \"mlm_wnre\": \"65,105,225\"\n",
    "}\n",
    "\n",
    "m2 = \"mlm_only\"\n",
    "m1 = \"mlm_wnre\"\n",
    "fig = plot_intruding_preds(\n",
    "    'hypernym',\n",
    "    m1,m2,\n",
    "    MODEL_LEGENDS[m1], MODEL_LEGENDS[m2],\n",
    "    MODEL_RGBS[m1], MODEL_RGBS[m2]\n",
    ")\n",
    "fig.update_yaxes(type=\"linear\")\n",
    "fig.update_xaxes(type=\"linear\")\n",
    "\n",
    "fig.update_layout(showlegend=True, template='simple_white')\n",
    "fig.update_layout(legend=dict(yanchor='bottom', xanchor='right', x=0.99,y=0.01))\n",
    "\n",
    "\n",
    "fig.update_layout(width =500, height=250, \n",
    "                  font_family=\"Serif\", \n",
    "                  font_size=12, \n",
    "                  margin_l=5, margin_t=5, margin_b=5, margin_r=5)\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Number of training examples\",\n",
    "    yaxis_title=\"Class Intrusion Rate\",\n",
    ")\n",
    "display(HTML(fig.to_html()))\n",
    "pio.write_image(fig, \"ft_hypcc_intrusion_hypernym.pdf\", width=1.5*300, height=0.75*300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LEGENDS = {\n",
    "    \"mlm_only\": \"BERT (OURS)\",\n",
    "    \"mlm_wnre\": \"BERT+BALAUR\"\n",
    "}\n",
    "MODEL_RGBS = {\n",
    "    \"mlm_only\": \"255,127,80\",\n",
    "    \"mlm_wnre\": \"65,105,225\"\n",
    "}\n",
    "\n",
    "m2 = \"mlm_only\"\n",
    "m1 = \"mlm_wnre\"\n",
    "fig = plot_intruding_preds(\n",
    "    'hyponym',\n",
    "    m1,m2,\n",
    "    MODEL_LEGENDS[m1], MODEL_LEGENDS[m2],\n",
    "    MODEL_RGBS[m1], MODEL_RGBS[m2]\n",
    ")\n",
    "fig.update_yaxes(type=\"linear\")\n",
    "fig.update_xaxes(type=\"linear\")\n",
    "\n",
    "fig.update_layout(showlegend=True, template='simple_white')\n",
    "fig.update_layout(legend=dict(yanchor='bottom', xanchor='right', x=0.99,y=0.01))\n",
    "\n",
    "\n",
    "fig.update_layout(width =500, height=250, \n",
    "                  font_family=\"Serif\", \n",
    "                  font_size=12, \n",
    "                  margin_l=5, margin_t=5, margin_b=5, margin_r=5)\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Number of training examples\",\n",
    "    yaxis_title=\"Class Intrusion Rate\",\n",
    ")\n",
    "display(HTML(fig.to_html()))\n",
    "pio.write_image(fig, \"ft_hypcc_intrusion_hyponym.pdf\", width=1.5*300, height=0.75*300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = tr.AutoTokenizer.from_pretrained('roberta-base')\n",
    "class_counters = {}\n",
    "for k, d in tokenized.items():\n",
    "    c = Counter(chain.from_iterable(d['labels']))\n",
    "    class_counters[k] = c\n",
    "\n",
    "def convert_token(token_id: int) -> str:\n",
    "    token = tknzr.convert_ids_to_tokens(token_id)\n",
    "    if token[0] == 'Ä ':\n",
    "        token = token[1:]\n",
    "    return token\n",
    "\n",
    "def error_analysis(task: str, model1: str, model2: str, leg1: str, leg2: str, rgb1: str='45,114,178', rgb2: str='194,24,7'):\n",
    "    traces = []\n",
    "    for obj, leg, rgb in zip([model1, model2], [leg1, leg2], [rgb1,rgb2]):\n",
    "        x = []\n",
    "        y = []\n",
    "        \n",
    "        n = 0\n",
    "        intrusion_rate = defaultdict(int)        \n",
    "        for seed in eval_metrics[task][obj]:\n",
    "            # do this only for last step\n",
    "            step = max(eval_metrics[task][obj][seed].keys())\n",
    "            metric = eval_metrics[task][obj][seed][step]\n",
    "            for i, preds in enumerate(metric.top10_labels):\n",
    "                for rank, pred in enumerate(preds):\n",
    "                    if pred not in metric.labels[i] and pred in tokenized_categories[task]:\n",
    "                        intrusion_rate[pred] += 1\n",
    "                n += 1\n",
    "        intrusion_rate = {k:v/n for k,v in intrusion_rate.items()}    \n",
    "                \n",
    "        idx = [i for i,_ in class_counters[task].most_common() if i in intrusion_rate]\n",
    "        x = [class_counters[task][i] for i in idx]\n",
    "        y = [intrusion_rate[i] for i in idx]\n",
    "        z = [convert_token(i) for i in idx]\n",
    "        traces.extend(plot_trace(x,y,leg,rgb, mode='markers', z=z))\n",
    "    fig = go.Figure(traces)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LEGENDS = {\n",
    "    \"mlm_only\": \"BERT (OURS)\",\n",
    "    \"mlm_wnre\": \"BERT+BALAUR\"\n",
    "}\n",
    "MODEL_RGBS = {\n",
    "    \"mlm_only\": \"255,127,80\",\n",
    "    \"mlm_wnre\": \"65,105,225\"\n",
    "}\n",
    "\n",
    "m2 = \"mlm_only\"\n",
    "m1 = \"mlm_wnre\"\n",
    "fig = error_analysis(\n",
    "    'hypernym',\n",
    "    m1,m2,\n",
    "    MODEL_LEGENDS[m1], MODEL_LEGENDS[m2],\n",
    "    MODEL_RGBS[m1], MODEL_RGBS[m2]\n",
    ")\n",
    "\n",
    "fig.update_yaxes(type=\"linear\")\n",
    "fig.update_xaxes(type=\"log\")\n",
    "\n",
    "fig.update_layout(showlegend=True, template='simple_white')\n",
    "fig.update_layout(legend=dict(yanchor='bottom', xanchor='right', x=0.4,y=0.7))\n",
    "\n",
    "\n",
    "fig.update_layout(width =500, height=250, \n",
    "                  font_family=\"Serif\", \n",
    "                  font_size=12, \n",
    "                  margin_l=5, margin_t=5, margin_b=5, margin_r=5)\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Class Frequency in HypCC\",\n",
    "    yaxis_title=\"Intrusion Rate\",\n",
    ")\n",
    "display(HTML(fig.to_html()))\n",
    "pio.write_image(fig, \"ft_hypcc_freq-intrusion_hypernym.pdf\", width=1.5*300, height=0.75*300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LEGENDS = {\n",
    "    \"mlm_only\": \"BERT (OURS)\",\n",
    "    \"mlm_wnre\": \"BERT+BALAUR\"\n",
    "}\n",
    "MODEL_RGBS = {\n",
    "    \"mlm_only\": \"255,127,80\",\n",
    "    \"mlm_wnre\": \"65,105,225\"\n",
    "}\n",
    "\n",
    "m2 = \"mlm_only\"\n",
    "m1 = \"mlm_wnre\"\n",
    "fig = error_analysis(\n",
    "    'hyponym',\n",
    "    m1,m2,\n",
    "    MODEL_LEGENDS[m1], MODEL_LEGENDS[m2],\n",
    "    MODEL_RGBS[m1], MODEL_RGBS[m2]\n",
    ")\n",
    "\n",
    "fig.update_yaxes(type=\"linear\")\n",
    "fig.update_xaxes(type=\"log\")\n",
    "\n",
    "fig.update_layout(showlegend=True, template='simple_white')\n",
    "fig.update_layout(legend=dict(yanchor='bottom', xanchor='right', x=0.4,y=0.7))\n",
    "\n",
    "\n",
    "fig.update_layout(width =500, height=250, \n",
    "                  font_family=\"Serif\", \n",
    "                  font_size=12, \n",
    "                  margin_l=5, margin_t=5, margin_b=5, margin_r=5)\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Class Frequency in HypCC\",\n",
    "    yaxis_title=\"Intrusion Rate\",\n",
    ")\n",
    "display(HTML(fig.to_html()))\n",
    "pio.write_image(fig, \"ft_hypcc_freq-intrusion_hyponym.pdf\", width=1.5*300, height=0.75*300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (blr)",
   "language": "python",
   "name": "blr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
